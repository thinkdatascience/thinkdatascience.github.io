---
title: Solving Multicollinearity using Principal Component Analysis 
author: Akshay Adlakha & Akshaykumar Rao
date: 2020-12-23 08:10:00 +0800
categories: [Blogging, Tutorial]
tags: [writing]
math: true
mermaid: true
---

Principal Component Analysis, PCA is a unsupervised learning technique for dimensionality reduction. What does it mean?
Imagine you have collected a dataset that has 100s of features. As you could not have kept track of all these 100+ features while collecting the dataset, there's a high chance of presence of multicollinearity in the dataset. In such case, PCA can be used to transform such as large set of variables into a smaller set while still retaining most of the information in the large set. 

## What is the basic idea behind PCA ?
As it is impossible to visualize a 100-dimensional data, we will try to reduce 2D data to 1D data.
Consider our example below (figure (A)), we know that price of the house increases with increase in its area.

![PCA-idea](/assets/img/sample/PCA_idea.JPG)

To reduce the data from 2D to 1D, we try to find a line on which our data points seem to lie (figure (B)), and then we project out data points onto that line. The line is chosen such that the sum of distances between the data point and its projection on the line (called projection error) is small. Thus, we create a new feature Z1 where we project our data points ((figure (C)). So now we need only one number to represent the position of the data points.

So, to reduce our data from d-dimension to k-dimension, we find 'k' vectors onto which we project our data to, so as to minimize the projection error. 

To read more about the steps involved in PCA, Check out this [blog](https://thinkdatascience.github.io/posts/write-a-new-post/).


## What are the advantages of performing PCA?

- Data Compression

As we have seen in our example before we can project 2D data onto a line so that we no longer need 2 numbers to represent out data, we will need only one. Similarly, we can try to project 3D data into a 2D plane on which our data seem to lie on and we will then need only 2 numbers to represent our data points. Compressing data will allow us to run our learning algorithms faster.

- Data Visualization

Sometimes it may be useful to understand the data better by plotting it. However, it is impossible to visualise our data when we are dealing with 50+ features. In such case, PCA can help us reduce our data from 50D to 2D/3D making it easier to visualize.

- Reducing Multicollinearity

PCA solves one of the biggest problems in large datasets - Multicollinearity. It removes highly correlated features by keeping most of information. 

## Python Implementation

Let's see how we can deal with multicollinearity in our data using PCA.

### Importing Libraries


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
```

### Loading Boston Housing Dataset


```python
from sklearn.datasets import load_boston
data = load_boston()
```


```python
X = data['data']
y = data['target']
cols = list(data['feature_names'])
```


### Check if there is multicollinearity using VIF


```python
df = pd.DataFrame(X,columns=cols)
```


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
    </tr>
  </tbody>
</table>
</div>
<br>




```python
vif = pd.DataFrame()
vif["Columns"] = df.columns
vif['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
vif
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Columns</th>
      <th>VIF</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>CRIM</td>
      <td>2.100373</td>
    </tr>
    <tr>
      <th>1</th>
      <td>ZN</td>
      <td>2.844013</td>
    </tr>
    <tr>
      <th>2</th>
      <td>INDUS</td>
      <td>14.485758</td>
    </tr>
    <tr>
      <th>3</th>
      <td>CHAS</td>
      <td>1.152952</td>
    </tr>
    <tr>
      <th>4</th>
      <td>NOX</td>
      <td>73.894947</td>
    </tr>
    <tr>
      <th>5</th>
      <td>RM</td>
      <td>77.948283</td>
    </tr>
    <tr>
      <th>6</th>
      <td>AGE</td>
      <td>21.386850</td>
    </tr>
    <tr>
      <th>7</th>
      <td>DIS</td>
      <td>14.699652</td>
    </tr>
    <tr>
      <th>8</th>
      <td>RAD</td>
      <td>15.167725</td>
    </tr>
    <tr>
      <th>9</th>
      <td>TAX</td>
      <td>61.227274</td>
    </tr>
    <tr>
      <th>10</th>
      <td>PTRATIO</td>
      <td>85.029547</td>
    </tr>
    <tr>
      <th>11</th>
      <td>B</td>
      <td>20.104943</td>
    </tr>
    <tr>
      <th>12</th>
      <td>LSTAT</td>
      <td>11.102025</td>
    </tr>
  </tbody>
</table>
</div>


As we see there is high collinearity in the dataset. Now we will implement a linear model.

### Implementing Linear Model and see multicollinearity


```python
X_Const = sm.add_constant(X)
```


```python
X_Const
```




    array([[1.0000e+00, 6.3200e-03, 1.8000e+01, ..., 1.5300e+01, 3.9690e+02,
            4.9800e+00],
           [1.0000e+00, 2.7310e-02, 0.0000e+00, ..., 1.7800e+01, 3.9690e+02,
            9.1400e+00],
           [1.0000e+00, 2.7290e-02, 0.0000e+00, ..., 1.7800e+01, 3.9283e+02,
            4.0300e+00],
           ...,
           [1.0000e+00, 6.0760e-02, 0.0000e+00, ..., 2.1000e+01, 3.9690e+02,
            5.6400e+00],
           [1.0000e+00, 1.0959e-01, 0.0000e+00, ..., 2.1000e+01, 3.9345e+02,
            6.4800e+00],
           [1.0000e+00, 4.7410e-02, 0.0000e+00, ..., 2.1000e+01, 3.9690e+02,
            7.8800e+00]])




```python
linearmodel = sm.OLS(y,X_Const)
```


```python
modelresult = linearmodel.fit()
```


```python
print(modelresult.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                      y   R-squared:                       0.741
    Model:                            OLS   Adj. R-squared:                  0.734
    Method:                 Least Squares   F-statistic:                     108.1
    Date:                Wed, 23 Dec 2020   Prob (F-statistic):          6.72e-135
    Time:                        14:47:30   Log-Likelihood:                -1498.8
    No. Observations:                 506   AIC:                             3026.
    Df Residuals:                     492   BIC:                             3085.
    Df Model:                          13                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const         36.4595      5.103      7.144      0.000      26.432      46.487
    x1            -0.1080      0.033     -3.287      0.001      -0.173      -0.043
    x2             0.0464      0.014      3.382      0.001       0.019       0.073
    x3             0.0206      0.061      0.334      0.738      -0.100       0.141
    x4             2.6867      0.862      3.118      0.002       0.994       4.380
    x5           -17.7666      3.820     -4.651      0.000     -25.272     -10.262
    x6             3.8099      0.418      9.116      0.000       2.989       4.631
    x7             0.0007      0.013      0.052      0.958      -0.025       0.027
    x8            -1.4756      0.199     -7.398      0.000      -1.867      -1.084
    x9             0.3060      0.066      4.613      0.000       0.176       0.436
    x10           -0.0123      0.004     -3.280      0.001      -0.020      -0.005
    x11           -0.9527      0.131     -7.283      0.000      -1.210      -0.696
    x12            0.0093      0.003      3.467      0.001       0.004       0.015
    x13           -0.5248      0.051    -10.347      0.000      -0.624      -0.425
    ==============================================================================
    Omnibus:                      178.041   Durbin-Watson:                   1.078
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
    Skew:                           1.521   Prob(JB):                    8.84e-171
    Kurtosis:                       8.281   Cond. No.                     1.51e+04
    ==============================================================================
    
    Warnings:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 1.51e+04. This might indicate that there are
    strong multicollinearity or other numerical problems.


From the above results, we can see that the condition number is large. There are high chances of multicollinearity in the dataset irrespective of good R-Squared and Adjusted R-Squared value. Lets see how PCA can remove multicollinearity from the dataset.


### Feature Scaling to implement PCA


```python
sc = StandardScaler()
X_std = sc.fit_transform(X)
```

### Implementing PCA


```python
pca = PCA()
X_std = pca.fit_transform(X_std)
explained_variance = pca.explained_variance_ratio_
```


```python
explained_variance
```




    array([0.47129606, 0.11025193, 0.0955859 , 0.06596732, 0.06421661,
           0.05056978, 0.04118124, 0.03046902, 0.02130333, 0.01694137,
           0.0143088 , 0.01302331, 0.00488533])




```python
plt.figure()
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Variance')
plt.title('Boston housing dataset')
plt.show()
```


![plot](/assets/img/sample/pca.png)


```python
np.cumsum(pca.explained_variance_ratio_)
```




    array([0.47129606, 0.581548  , 0.67713389, 0.74310121, 0.80731782,
           0.8578876 , 0.89906884, 0.92953786, 0.9508412 , 0.96778257,
           0.98209137, 0.99511467, 1.        ])


Here we can say that 8 components out of 12 are retaining 95% of the information. So, we can remove the rest of the components. 

### PCA with 8 components


```python
pca = PCA(n_components = 8)
```


```python
X_pca = pca.fit_transform(X_std)
```


```python
X_pca
```




    array([[-2.09829747,  0.77311275,  0.34294273, ..., -0.31533814,
             0.31864075,  0.2958318 ],
           [-1.45725167,  0.59198521, -0.69519931, ...,  0.26422321,
             0.55386126, -0.22366994],
           [-2.07459756,  0.5996394 ,  0.1671216 , ...,  0.44809462,
             0.48455996,  0.10516613],
           ...,
           [-0.31236047,  1.15524644, -0.40859759, ...,  0.46794669,
             0.29411936, -0.63866037],
           [-0.27051907,  1.04136158, -0.58545406, ...,  0.48225947,
             0.27159707, -0.57934447],
           [-0.12580322,  0.76197805, -1.294882  , ...,  0.40147353,
             0.17530965, -0.13338197]])




```python
X_Const_pca = sm.add_constant(X_pca)
```


```python
linearmodelpca = sm.OLS(y,X_Const_pca)
```


```python
modelresultpca = linearmodelpca.fit()
```


```python
print(modelresultpca.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                      y   R-squared:                       0.706
    Model:                            OLS   Adj. R-squared:                  0.701
    Method:                 Least Squares   F-statistic:                     148.9
    Date:                Wed, 23 Dec 2020   Prob (F-statistic):          9.95e-127
    Time:                        14:47:30   Log-Likelihood:                -1530.9
    No. Observations:                 506   AIC:                             3080.
    Df Residuals:                     497   BIC:                             3118.
    Df Model:                           8                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const         22.5328      0.224    100.763      0.000      22.093      22.972
    x1            -2.2708      0.090    -25.135      0.000      -2.448      -2.093
    x2             2.1927      0.187     11.739      0.000       1.826       2.560
    x3             3.4975      0.201     17.435      0.000       3.103       3.892
    x4            -1.0796      0.241     -4.471      0.000      -1.554      -0.605
    x5            -2.2309      0.245     -9.115      0.000      -2.712      -1.750
    x6            -0.6700      0.276     -2.429      0.015      -1.212      -0.128
    x7            -0.0942      0.306     -0.308      0.758      -0.695       0.506
    x8             1.0392      0.355      2.925      0.004       0.341       1.737
    ==============================================================================
    Omnibus:                      224.135   Durbin-Watson:                   0.975
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1490.162
    Skew:                           1.810   Prob(JB):                         0.00
    Kurtosis:                      10.588   Cond. No.                         3.93
    ==============================================================================
    
    Warnings:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


### Conclusion

After implementing PCA, we see that there is difference between the model with multicollinearity and without multicollinearity. And, we are not getting a warning message. So, there is no multicollinearity in the dataset.


