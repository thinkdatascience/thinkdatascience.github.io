---
title: How to interpret your Machine Learning model using LIME?
author: Akshay Adlakha & Akshaykumar Rao
date: 2020-12-24 06:55:00 +0800
categories: [Machine Learning, Python]
tags: [Machine Learning]
math: true 
mermaid: true
---


## Machine Learning Models interpretation

The most critical part of Machine Learning model life cycle is to interpret models after training. Sometimes, we get an underfit or overfit model. In this post, we will see how we can easily interpret our model. 

![upload-image](/assets/img/sample/modelinter1.jpg)

## Agenda

- Introduction to LIME
- Python Implemention
  - Model Training 
  - Model Interpretation
- Conclusion
 
For any machine learning project, it is really important for tweaking to find out why the model makes predictions the way it does. So, Interpreting models and the importance of each predictor should become second nature. LIME will make this easy to interpret machine learning model.

LIME isn’t the only library for interpreting machine learning model. We have got an alternative one -  SHAP. You can learn more about it [here.](https://thinkdatascience.github.io/posts/Interpretation_using_SHAP/)

Let's see what actually is:

## Introduction to LIME

LIME stands for Local Interpretable Model-agnostic Explanations. It helps in understaning Tabular Models, Image and Text Classifiers. This gives an brief explanation of what each predictor is doing in the prediction and lists out what features are contributing positively or negatively.

To use LIME, you need to install it through the terminal.
 
 ```python
    pip install lime
 ```
 
LIME explains how our model is behaving. If a model is not doing what it intends to do, there might be a good chance that you have done some mistake in the data preprocessing. And, if we know what we did wrong, we can easily correct it. 

Lets see how we can use LIME to interpret our machine learning models.

## Python Implementation

Lets start by importing necessary libraries.

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
```

Here, we are reading our data. You can get this Real Estate valuation data from [here.](https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set)

```python
data = pd.read_excel('Realestate.xlsx')
```


```python
data.head()
```


We have 6 input features and our target feature - Y house price of unit area.

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>X1 transaction date</th>
      <th>X2 house age</th>
      <th>X3 distance to the nearest MRT station</th>
      <th>X4 number of convenience stores</th>
      <th>X5 latitude</th>
      <th>X6 longitude</th>
      <th>Y house price of unit area</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2012.916667</td>
      <td>32.0</td>
      <td>84.87882</td>
      <td>10</td>
      <td>24.98298</td>
      <td>121.54024</td>
      <td>37.9</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2012.916667</td>
      <td>19.5</td>
      <td>306.59470</td>
      <td>9</td>
      <td>24.98034</td>
      <td>121.53951</td>
      <td>42.2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2013.583333</td>
      <td>13.3</td>
      <td>561.98450</td>
      <td>5</td>
      <td>24.98746</td>
      <td>121.54391</td>
      <td>47.3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2013.500000</td>
      <td>13.3</td>
      <td>561.98450</td>
      <td>5</td>
      <td>24.98746</td>
      <td>121.54391</td>
      <td>54.8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2012.833333</td>
      <td>5.0</td>
      <td>390.56840</td>
      <td>5</td>
      <td>24.97937</td>
      <td>121.54245</td>
      <td>43.1</td>
    </tr>
  </tbody>
</table>
</div>
<br>


Now we take out the target feature.

```python
y = data['Y house price of unit area']
X = data.drop('Y house price of unit area', axis=1)
```

Here, we split our data into training and testing set.

```python
X_train,X_test, Y_train, Y_test = train_test_split(X,
                                                   y,
                                                   test_size =0.2,
                                                   random_state=0
                                                )
```

Lets build our Random Forest Regressor model.

```python
from sklearn.ensemble import RandomForestRegressor
```


```python
model = RandomForestRegressor(random_state = 0,
                             n_estimators=100)
model.fit(X_train,Y_train)
```




    RandomForestRegressor(random_state=0)


So, we have build our model. It is time to see the implementation of LIME. As we are using a tabular data. So, we will import `lime_tabular` from the lime package.

```python
from lime import lime_tabular
```

We will use the `LimeTabularExplainer` to explain the behaviour of our Random Forest model. Here we are passing our training data in the form of array because Lime only accepts numpy array.

```python
explainer = lime_tabular.LimeTabularExplainer(training_data= np.array(X_train),
                                             mode='regression',
                                             feature_names=X_train.columns)
```

Now, we will see an explaination for the 5th instance in our test set. 

```python
result = explainer.explain_instance(data_row=X_test.iloc[5],
                                   predict_fn=model.predict)
```

```python
result.show_in_notebook(show_table=True)
```

![upload-image](/assets/img/sample/lime1.png)


From the above results, we can see that our model predicted a price of 38.93 per unit area. It is also showing us that which features contributed positively or negatively. X2 feature had a value 36.60. This results say that a value above 26.85 has a negative impact of 5.50 on the prediction. In similar way, a value above 289.32 for X3 feature is having a significant impact. Now, we know the contribution of our freatures and can interpret our results easily. This helps in understanding our business problem and reaching out to the goal faster. 


```python
result.as_list()
```




    [('289.32 < X3 distance to the nearest MRT station <= 512.79',
      6.773654982050966),
     ('X2 house age > 26.85', -5.368871617705421),
     ('X6 longitude > 121.54', 0.4494951909612462),
     ('No > 312.50', -0.431652062202576),
     ('2012.92 < X1 transaction date <= 2013.17', 0.21873114317211426),
     ('X4 number of convenience stores > 6.00', 0.021485525788385217),
     ('24.96 < X5 latitude <= 24.97', 0.012082440378980944)]


We will the result of another instance. A model predicted a house price of 50.08 per unit area. Most of the features had a positive impact on the prediction.


```python
result20 = explainer.explain_instance(data_row=X_test.iloc[20],
                                   predict_fn=model.predict)
```


```python
result20.show_in_notebook(show_table=True)
```

![upload-image](/assets/img/sample/lime2.png)



```python
result20.as_list()
```




    [('X3 distance to the nearest MRT station <= 289.32', 12.293529250282159),
     ('X2 house age <= 8.70', 9.121892110200877),
     ('24.97 < X5 latitude <= 24.98', 1.200824769535087),
     ('X1 transaction date <= 2012.92', -0.8941765157153577),
     ('121.54 < X6 longitude <= 121.54', -0.8804370606024536),
     ('No > 312.50', 0.3360695047391088),
     ('X4 number of convenience stores > 6.00', 0.06323751180901528)]


## Conclusion

Interpreting machine learning models is quite simple with LIME. It provides you a simple and easy way of explaining what’s going on below the surface to non-technical people. And, we don't even need worry about visualization. LIME has a lot different visualizations. Moreover, it not only works with tabular data, you can interpret the text and image data as well.   

