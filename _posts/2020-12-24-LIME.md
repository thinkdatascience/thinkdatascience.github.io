---
title: How to interpret your Machine Learning model using LIME?
author: Akshay Adlakha & Akshaykumar Rao
date: 2020-12-24 06:55:00 +0800
categories: [Machine Learning, Python]
tags: [Machine Learning]
math: true 
mermaid: true
---


## Machine Learning Models interpretation

The most critical part of Machine Learning model life cycle is to interpret models after training. Sometimes, we get an underfit or overfit model. In this post, we will see how we can easily interpret our model. 

![upload-image](/assets/img/sample/modelinter1.jpeg)

## Agenda

- Introduction to LIME
- Python Implemention
  - Model Training 
  - Model Interpretation
- Conclusion
 
For any machine learning project, it is really important for tweaking to find out why the model makes predictions the way it does. So, Interpreting models and the importance of each predictor should become second nature. LIME will make this easy to interpret machine learning model.

LIME isnâ€™t the only library for interpreting machine learning model. We have got an alternative one -  SHAP. You can learn more about it here:

Let's see what actually is:

## Introduction to LIME

LIME stands for Local Interpretable Model-agnostic Explanations. It helps in understaning Tabular Models, Image and Text Classifiers. This gives an brief explanation of what each predictor is doing in the prediction and lists out what features are contributing positively or negatively.

To use LIME, you need to install it through the terminal.
 
 ```python
    pip install lime
 ```
 
LIME explains how our model is behaving. If a model is not doing what it intends to do, there might be a good chance that you have done some mistake in the data preprocessing. And, if we know what we did wrong, we can easily correct it. 
